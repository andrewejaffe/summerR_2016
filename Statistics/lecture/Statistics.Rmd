---
title: "Statistics"
author: "Andrew Jaffe, John Muschelli"
date: "January 8, 2015"
output:
  beamer_presentation: default
  ioslides_presentation:
    css: ../../styles.css
  slidy_presentation: default
---

## Statistics

Now we are going to cover how to perform a variety of basic statistical tests in R. 

* Correlation
* T-tests
* Linear Regression
* Logistic Regression
* Proportion tests
* Chi-squared
* Fisher's Exact Test

Note: We will be glossing over the statistical theory and "formulas" for these tests. There are plenty of resources online for learning more about these tests, as well as dedicated Biostatistics series at the School of Public Health

## Correlation {.smaller}

`cor()` performs correlation in R

```
cor(x, y = NULL, use = "everything",
    method = c("pearson", "kendall", "spearman"))
```

Like other functions, if there are NAs, you get NA as the result.  But if you specify use only the complete observations, then it will give you correlation on the non-missing data. 
```{r cor1, comment="",prompt=TRUE}
circ = read.csv("http://www.aejaffe.com/winterR_2016/data/Charm_City_Circulator_Ridership.csv", 
           header=TRUE,as.is=TRUE)
cor(circ$orangeAverage, circ$purpleAverage)
cor(circ$orangeAverage, circ$purpleAverage, use="complete.obs")
```

## Correlation  {.smaller}

You can also get the correlation between matrix columns

```{r cor2, comment="",prompt=TRUE}
signif(cor(circ[,grep("Average",names(circ))], 
            use="complete.obs"),3)
```

## Correlation  {.smaller}

You can also get the correlation between matrix columns

Or between columns of two matrices, column by column.

```{r cor3, comment="",prompt=TRUE}
signif(cor(circ[,3:4],circ[,5:6], use="complete.obs"),3)
```

## Correlation  {.smaller}

You can also use cor.test() to test for whether correlation is significant (ie non-zero). Note that linear regression may be better, especially if you want to regress out other confounders.  

```{r cor4, comment="",prompt=TRUE}
ct= cor.test(circ$orangeAverage,
    circ$purpleAverage, use="complete.obs")
ct
```

## Correlation {.smaller}

Note that you can add the correlation to a plot, via the legend() function. 

```{r cor4a, comment="",prompt=TRUE, fig.height=4,fig.width=4}
plot(circ$orangeAverage, circ$purpleAverage,
     xlab="Orange Line", ylab="Purple Line",
     main="Average Ridership",cex.axis=1.5,
     cex.lab=1.5,cex.main=2)
legend("topleft", paste0("r=", signif(ct$estimate,3)), 
       bty="n",cex=1.5)
```

## Correlation

For many of these testing result objects, you can extract specific slots/results as numbers, as the `ct` object is just a list.

```{r cor5, comment="",prompt=TRUE}
# str(ct)
names(ct)
ct$statistic
ct$p.value
```

## T-tests

The T-test is performed using the `t.test()` function, which essentially tests for the difference in means of a variable between two groups.

In this syntax, x and y are the column of data for each group.

```{r tt1, comment="",prompt=TRUE}
tt = t.test(circ$orangeAverage, circ$purpleAverage)
tt
```

## T-tests
`t.test` saves a lot of information: the difference in means `estimate`, confidence interval for the difference `conf.int`, the p-value `p.value`, etc.
```{r tt1_1, comment="", prompt=TRUE}
names(tt)
```

## T-tests

You can also use the 'formula' notation.  In this syntax, it is `y ~ x`, where `x` is a factor with 2 levels or a binary variable and `y` is a vector of the same length.  


```{r tt2, comment="",prompt=TRUE,cache=TRUE}
http_data_dir = "http://www.aejaffe.com/winterR_2016/data/"
cars = read.csv(paste0(http_data_dir, "kaggleCarAuction.csv"),
                as.is=TRUE)
tt2 = t.test(VehBCost~IsBadBuy, data=cars)
tt2$estimate
```

## T-tests

You can add the t-statistic and p-value to a boxplot.

```{r tt3, comment="",prompt=TRUE, fig.height=4,fig.width=4,cache=TRUE}
boxplot(VehBCost~IsBadBuy, data=cars, 
        xlab="Bad Buy",ylab="Value")
leg = paste("t=", signif(tt$statistic,3), 
    " (p=",signif(tt$p.value,3),")",sep="")
legend("topleft", leg, cex=1.2, bty="n")
```

## Linear Regression

Now we will briefly cover linear regression. I will use a little notation here so some of the commands are easier to put in the proper context.
$$
y_i = \alpha + \beta x_i + \varepsilon_i 
$$
where:

* $y_i$ is the outcome for person i
* $\alpha$ is the intercept
* $\beta$ is the slope
* $x_i$ is the predictor for person i
* $\varepsilon_i$ is the residual variation for person i

## Linear Regression

The `R` version of the regression model is:

```
y ~ x
```

where: 

* y is your outcome
* x is/are your predictor(s)

## Linear Regression

For a linear regression, when the predictor is binary this is the same as a t-test:

```{r regress1, comment="",prompt=TRUE}
fit = lm(VehBCost~IsBadBuy, data=cars)
fit
```

'(Intercept)' is $\alpha$

'IsBadBuy' is $\beta$

## Linear Regression

The `summary` command gets all the additional information (p-values, t-statistics, r-square) that you usually want from a regression.

```{r regress2, comment="",prompt=TRUE}
sfit = summary(fit)
print(sfit)
```

## Linear Regression

The coefficients from a `summary` are the coefficients, standard errors, t-statistcs, and p-values for all the estimates.

```{r regress3, comment="",prompt=TRUE}
names(sfit)
sfit$coef
```

## Linear Regression

We'll look at vehicle odometer value by vehicle age:

```{r}
fit = lm(VehOdo~VehicleAge, data=cars)
print(fit)
```

## Linear Regression {.smaller}

We can visualize the vehicle age/odometer relationshp using scatter plots or box plots (with regression lines).  The function `abline` will plot the regresion line on the plot.

## Linear Regression
```{r regress4, comment="",prompt=TRUE, fig.height=4,fig.width=8,cache=TRUE}
library(scales) # we need this for the alpha command - make points transparent
par(mfrow=c(1,2))
plot(VehOdo ~ jitter(VehicleAge,amount=0.2), data=cars, pch = 19,
     col = alpha("black",0.05), xlab="Vehicle Age (Yrs)")
abline(fit, col="red",lwd=2)
legend("topleft", paste("p =",summary(fit)$coef[2,4]))
boxplot(VehOdo ~ VehicleAge, data=cars, varwidth=TRUE)
abline(fit, col="red",lwd=2)
```

## Linear Regression

Note that you can have more than 1 predictor in regression models.The interpretation for each slope is change in the predictor corresponding to a one-unit change in the outcome, holding all other predictors constant.

```{r regress5, comment="",prompt=TRUE, fig.height=4,fig.width=8}
fit2 = lm(VehOdo ~ IsBadBuy + VehicleAge, data=cars)
summary(fit2)  
```

## Linear Regression

Added-Variable plots can show you the relationship between a variable and outcome after adjusting for other variables.  The function `avPlots` from the `car` package can do this:

```{r avplot, comment="",prompt=TRUE, fig.height=4,fig.width=8}
library(car)
avPlots(fit2)
```

## Linear Regression

Plot on an `lm` object will do diagnostic plots.  Residuals vs. Fitted should have no discernable shape (the red line is the smoother), the qqplot shows how well the residuals fit a normal distribution, and Cook's distance measures the influence of individual points. 

---

```{r plot_lm, comment="",prompt=TRUE, fig.height=4,fig.width=8}
par(mfrow=c(2,2))
plot(fit2, ask= FALSE)
```

## Linear Regression {.smaller}

Factors get special treatment in regression models - lowest level of the factor is the comparison group, and all other factors are relative to its values.

```{r regress6, comment="",prompt=TRUE, fig.height=4,fig.width=8}
fit3 = lm(VehOdo ~ factor(TopThreeAmericanName), data=cars)
summary(fit3)  
```

## Logistic Regression and GLMs {.smaller}

Generalized Linear Models (GLMs) allow for fitting regressions for non-continous/normal outcomes.  The `glm` has similar syntax to the `lm` command. Logistic regression is one example.

```{r regress7, comment="",prompt=TRUE, fig.height=4,fig.width=8}
glmfit = glm(IsBadBuy ~ VehOdo + VehicleAge, data=cars, family=binomial())
summary(glmfit)  
```

## Logistic Regression 

Note the coefficients are on the original scale, we must exponentiate them for odds ratios:

```{r regress8, comment="",prompt=TRUE, fig.height=4,fig.width=8}
exp(coef(glmfit))
```

## Proportion tests

`prop.test()` can be used for testing the null that the proportions (probabilities of success) in several groups are the same, or that they equal certain given values.

```
prop.test(x, n, p = NULL,
          alternative = c("two.sided", "less", "greater"),
          conf.level = 0.95, correct = TRUE)
```

```{r prop1, comment="",prompt=TRUE}
prop.test(x=15, n =32)
```

## Chi-squared tests

`chisq.test()` performs chi-squared contingency table tests and goodness-of-fit tests.

```
chisq.test(x, y = NULL, correct = TRUE,
           p = rep(1/length(x), length(x)), rescale.p = FALSE,
           simulate.p.value = FALSE, B = 2000)
```

```{r chisq1, comment="",prompt=TRUE}
tab = table(cars$IsBadBuy, cars$IsOnlineSale)
tab
```

## Chi-squared tests

You can also pass in a table object (such as `tab` here)
```{r chisq2, comment="",prompt=TRUE}
cq=chisq.test(tab)
cq
names(cq)
cq$p.value
```

## Chi-squared tests

Note that does the same test as `prop.test`, for a 2x2 table.

```{r chisq3, comment="",prompt=TRUE}
chisq.test(tab)
prop.test(tab)
```

## Fisher's Exact test

`fisher.test()` performs contingency table test using the hypogeometric distribution (used for small sample sizes).

```
fisher.test(x, y = NULL, workspace = 200000, hybrid = FALSE,
            control = list(), or = 1, alternative = "two.sided",
            conf.int = TRUE, conf.level = 0.95,
            simulate.p.value = FALSE, B = 2000)
```

```{r fish.test, comment="",prompt=TRUE}
fisher.test(tab)
```

## Probability Distributions

Sometimes you want to generate data from a distribution (such as normal), or want to see where a value falls in a known distribution. `R` has these distibutions built in:

* Normal
* Binomial
* Beta
* Exponential
* Gamma
* Hypergeometric
* etc

## Probability Distributions

Each has 4 options:

* `r` for random number generation [e.g. `rnorm()`]
* `d` for density [e.g. `dnorm()`]
* `p` for probability [e.g. `pnorm()`]
* `q` for quantile [e.g. `qnorm()`]

```{r rnorm, comment="",prompt=TRUE}
rnorm(5)
```

## Sampling

The `sample()` function is pretty useful for permutations

```{r sample, comment="",prompt=TRUE}
sample(1:10, 5, replace=FALSE)
```

## Sampling

Also, if you want to only plot a subset of the data (for speed/time or overplotting)

```{r samp_plot, comment="",prompt=TRUE, fig.height=4,fig.width=4}
samp.cars <- cars[ sample(nrow(cars), 10000), ]
plot(VehOdo ~ jitter(VehBCost,amount=0.3), data= samp.cars)  
```

## By popular demand

* Principal Component Analysis (PCA)
* Imputation

## PCA

Dimension reduction technique for identifying potentially hidden combinations of observed variables that explain variability in high dimensional data. 

Example from "R-bloggers": http://www.r-bloggers.com/computing-and-visualizing-pca-in-r/

## PCA

```{r}
data(iris)
head(iris, 3)
```

## PCA

```{r}
# log transform 
log.ir <- log(iris[, 1:4])
ir.species <- iris[, 5]
ir.pca <- prcomp(log.ir, center = TRUE, scale = TRUE) 
```

Centering - removing the mean - is almost always recommended, otherwise the first PC will represent the mean. Scaling is useful when columns/data are on different scales.

## PCA

So what does this do?

```{r}
print(ir.pca)
summary(ir.pca)
```

## PCA 

```{r}
plot(ir.pca$x, col = as.numeric(ir.species),pch=19)
legend("topleft", levels(ir.species), col=1:3, pch=15, nc=3)
```

## PCA 

```{r}
# devtools::install_github("ggbiplot", "vqv")
ggbiplot::ggbiplot(ir.pca, obs.scale = 1, var.scale = 1, 
    groups = ir.species, ellipse = TRUE, 
    circle = TRUE) + scale_color_discrete(name = '') +
    theme(legend.direction = 'horizontal', 
               legend.position = 'top')
```

## Multiple Imputation

Sometimes you want to "impute" missing data in a dataset if its missing completely at random (MCAR).

There are several methods for performing imputation in a dataset, but one popular approach is "multiple imputation", which performs several imputation procedures and then takes the average across these runs. 
Let's take a look at the `mi` package. This is motivated by this guide: http://thomasleeper.com/Rcourse/Tutorials/mi.html

## Multiple Imputation

```{r}
set.seed(210)
x = c(sample(1:20, 16, TRUE), rep(NA, 4))
x
mean(x, na.rm = TRUE)
sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x)))
```

## Multiple Imputation

```{r}
imp <- replicate(5, c(x[!is.na(x)], 
    sample(x[!is.na(x)], 4, TRUE)))
tail(imp)
colMeans(imp)
```

## Multiple Imputation

```{r}
overallMean = mean(colMeans(imp))
withinVar = mean(apply(imp,2,sd)/sqrt(length(x)))
betweenVar = sum((colMeans(imp) - overallMean)^2)/(5-1)
overallSE = sqrt(withinVar + ((1 + (1/5)) * betweenVar))

c(mean(1:20), overallMean, mean(x, na.rm = TRUE))
c(overallSE, sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))))
```

## Multiple Imputation

Let's look at some multivariate examples in the vignette for the `mi` package:

https://cran.r-project.org/web/packages/mi/vignettes/mi_vignette.pdf

